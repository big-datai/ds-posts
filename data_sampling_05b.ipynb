{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "### 1. Dealing with categorical features\n",
    "    1.1. Bucketing bins and One-Hot encoding\n",
    "    \n",
    "### 2. Feature transformation\n",
    "    2.1 log\n",
    "   \n",
    "    2.2 feature expentions, PolynomialFeatures, extraction\n",
    "    \n",
    "    2.3 Scaling or normalizing features within a range \n",
    "    \n",
    "### 3. Feature crossing - (long, lat) date,2-May-2019\n",
    "\n",
    "### 4. Row aggregation, group by\n",
    "\n",
    "    4.1 Moving averages with panda df\n",
    "\n",
    "### 5. Embedding - transformation, part of training. trainable/ non trainable, word2vec pre training. retrieve, train, seize, bottle neck tensor, calculate till you reach some training threshold, look up,feature columns.integers categorical.\n",
    "\n",
    "### 6. Simple model improved after feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project constants\n",
    "LEARN_SIZE = 50\n",
    "POLY_DEGREE = 2\n",
    "LEARNING_RATE = 0.001\n",
    "DISPLAY_STEP = 200\n",
    "NUM_STEPS = 3000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Immediate execution\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "From Google Machine Learning crash course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems reasonable to evaluate also the dataset features. The dataset is very abundant, so we may limit the data only for the evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = housing_df[:LEARN_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a reasonable task to start from the data check. Is all data numerical? Does it contain missed values? Pandas can help us with a handy check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before missing and corrupt values check:  (50, 9)\n",
      "Shape after missing and corrupt values check:  (50, 9)\n"
     ]
    }
   ],
   "source": [
    "print('Shape before missing and corrupt values check: ', housing_df.shape)\n",
    "\n",
    "# Force number conversion\n",
    "for col in list(housing_df):\n",
    "    housing_df[col] = pd.to_numeric(housing_df[col], errors='coerce')\n",
    "\n",
    "# Remove intrinsic and resulted NaN values\n",
    "housing_df.dropna(inplace=True)\n",
    "\n",
    "print('Shape after missing and corrupt values check: ', housing_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, all values are appropriate.\n",
    "\n",
    "We can continue with the Tensorflow data processing now. We may prepare all source dataset columns (numeric) as Feature Columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(c) for c in list(housing_df)]\n",
    "features = [{c: tf.convert_to_tensor(housing_df[c])} for c in list(housing_df)]\n",
    "nets = [tf.feature_column.input_layer(features[i], feature_columns[i]) for i in range(len(feature_columns))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dealing with categorical features\n",
    "1.1. Bucketing bins and One-Hot encoding\n",
    "1.2\n",
    "\n",
    "Some numeric values perform better in the categorized form. For example, we can convert 'Housing median age' field into bucketing form. Such operation results in the one-hot-encoded matrix with the parameters separated into predefined bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_index = list(housing_df).index('housing_median_age')\n",
    "\n",
    "bucketized_column = tf.feature_column.bucketized_column(\n",
    "    source_column = feature_columns[age_index],\n",
    "    boundaries = [10, 20, 30, 40])\n",
    "\n",
    "net_age_bucket = tf.feature_column.input_layer(features[age_index], bucketized_column)\n",
    "#print(net_age_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also a more condensed representation would help in our further calculations. This one-hot-decoding gives us a flat category (rank of current bin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_index = tf.reshape(tf.cast(tf.argmax(net_age_bucket, axis=1), tf.float32), [-1, 1])\n",
    "#print(bucket_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature transformation\n",
    "## 2.1 log\n",
    "Tensorflow helps us to make a direct conversion of feature data during the import. For example, we obtain a log transformed data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transformer(x):\n",
    "    return tf.cast(tf.log(x), dtype=tf.float32)\n",
    "\n",
    "log_feature_column_age = tf.feature_column.numeric_column('housing_median_age', normalizer_fn=log_transformer)\n",
    "net_age_log = tf.feature_column.input_layer(features[age_index], log_feature_column_age)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 feature expentions, PolynomialFeatures, extraction\n",
    "Data conversion may include a popular polynomial conversion. We will get the polynomially transformed (squared) data in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_transformer(x):\n",
    "    return tf.cast(tf.pow(x, POLY_DEGREE), dtype=tf.float32)\n",
    "\n",
    "poly_feature_column_age = tf.feature_column.numeric_column('housing_median_age', normalizer_fn=poly_transformer)\n",
    "net_age_poly = tf.feature_column.input_layer(features[age_index], poly_feature_column_age)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Scaling or normalizing features within a range \n",
    "Normalization is one of the most popular data transformation methods. Tensorflow helps us to prepare a feature column with zero mean and uniform standard deviation easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_mean = housing_df['housing_median_age'].mean()\n",
    "val_std = housing_df['housing_median_age'].std()\n",
    "\n",
    "def scaler(x):\n",
    "    return (tf.cast(x, dtype=tf.float32) - val_mean) / val_std\n",
    "\n",
    "scale_feature_column_age = tf.feature_column.numeric_column('housing_median_age', normalizer_fn=scaler)\n",
    "net_age_scale = tf.feature_column.input_layer(features[age_index], scale_feature_column_age)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature crossing - (long, lat) \n",
    "Need to mention also more complex data processing methods like feature crossing. They could help us to combine similar variables like longitude and latitude for a further simultaneous processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantile_based_boundaries(feature_values, num_buckets):\n",
    "    boundaries = np.arange(1.0, num_buckets) / num_buckets\n",
    "    quantiles = feature_values.quantile(boundaries)\n",
    "    return [quantiles[q] for q in quantiles.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-115.5,\n",
       " -115.49,\n",
       " -115.40299999999999,\n",
       " -115.374,\n",
       " -115.27,\n",
       " -114.676,\n",
       " -114.644,\n",
       " -114.6,\n",
       " -114.579]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_quantile_based_boundaries(housing_df['longitude'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32.68, 32.758, 32.81, 32.82, 33.055, 33.51, 33.61, 33.716, 34.577999999999996]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_quantile_based_boundaries(housing_df['latitude'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BucketizedColumn(source_column=NumericColumn(key='latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(32.68, 32.758, 32.81, 32.82, 33.055, 33.51, 33.61, 33.716, 34.577999999999996))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitude_bucket_feature_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=484, shape=(50, 50), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#todo explain the size of 50 vs 10\n",
    "longitude_bucket_feature_column = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column('longitude'),\n",
    "    boundaries=get_quantile_based_boundaries(housing_df['longitude'], 10))\n",
    "\n",
    "latitude_bucket_feature_column = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column('latitude'),\n",
    "    boundaries=get_quantile_based_boundaries(housing_df['latitude'], 10))\n",
    "\n",
    "crossed_lat_lon_feature_column = tf.feature_column.crossed_column(\n",
    "    [longitude_bucket_feature_column, latitude_bucket_feature_column], 50)\n",
    "crossed_column=tf.feature_column.indicator_column(crossed_lat_lon_feature_column)\n",
    "\n",
    "features = {'longitude': tf.convert_to_tensor(housing_df['longitude']), 'latitude': tf.convert_to_tensor(housing_df['latitude'])}\n",
    "feature_columns=[crossed_column]\n",
    "tf.feature_column.input_layer(features,feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Bucketize the latitude and longitude using the `edges`\n",
    "latitude_bucket_fc = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column('latitude'),\n",
    "    [1,2,3,4])\n",
    "\n",
    "longitude_bucket_fc = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column('longitude'),\n",
    "    [1,2,3,4])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BucketizedColumn(source_column=NumericColumn(key='longitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(1, 2, 3, 4))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longitude_bucket_fc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossedColumn(keys=(BucketizedColumn(source_column=NumericColumn(key='latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(1, 2, 3, 4)), BucketizedColumn(source_column=NumericColumn(key='longitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(1, 2, 3, 4))), hash_bucket_size=5000, hash_key=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossed_lat_lon_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossedColumn(keys=(BucketizedColumn(source_column=NumericColumn(key='longitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(-115.5, -115.49, -115.40299999999999, -115.374, -115.27, -114.676, -114.644, -114.6, -114.579)), BucketizedColumn(source_column=NumericColumn(key='latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(32.68, 32.758, 32.81, 32.82, 33.055, 33.51, 33.61, 33.716, 34.577999999999996))), hash_bucket_size=50, hash_key=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossed_lat_lon_feature_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Row aggregation, group by,\n",
    "A detailed dataset evaluation may include grouping of some features. What if we want to estimate average dataset values related to the same housing mean age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing_df.groupby(['housing_median_age']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Moving averages with panda df\n",
    "Otherwise we may change the dataset representation by preparing the moving average to smooth possible divagations of variable variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df_sorted = housing_df.sort_values('housing_median_age')\n",
    "housing_df_sorted['h_m_age_rolling'] = housing_df_sorted['housing_median_age'].rolling(5, min_periods=1).mean()\n",
    "housing_df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  5. Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. simple model improved after feature engineering.\n",
    "Let's make some tests with our processed data. Does the feature processing actually improves the prediction?\n",
    "\n",
    "First we need to define a really simple linear regression using Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of linear equation\n",
    "a = tfe.Variable(np.random.randn())\n",
    "b = tfe.Variable(np.random.randn())\n",
    "\n",
    "def linear_regression(inputs):\n",
    "    return inputs * a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression objective as minimization of error\n",
    "def mean_square_fn(model_fn, inputs, labels):\n",
    "    n_samples = int(tf.size(labels))\n",
    "    return tf.reduce_sum(tf.pow(model_fn(inputs) - labels, 2)) / (2 * n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "grad = tfe.implicit_gradients(mean_square_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main regression routine\n",
    "def make_regression(x, y):\n",
    "    for step in range(NUM_STEPS):\n",
    "        optimizer.apply_gradients(grad(linear_regression, x, y))\n",
    "        if (step + 1) % DISPLAY_STEP == 0 or step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (step + 1), \"cost=\",\n",
    "                  \"{:.9f}\".format(mean_square_fn(linear_regression, x, y)),\n",
    "                  \"a=\", a.numpy(), \"b=\", b.numpy())\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "def make_plot(x, y):\n",
    "    plt.plot(x, y, 'ro', label='Original')\n",
    "    plt.plot(x, np.array(a * x + b), label='Fitted')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to estimate median income using the source median housing age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_index = list(housing_df).index('median_income')\n",
    "\n",
    "a, b = make_regression(nets[age_index], nets[income_index])\n",
    "make_plot(nets[age_index], nets[income_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is also a good try to improve the accuracy, but unfortunately not in this case..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = make_regression(net_age_scale, nets[income_index])\n",
    "make_plot(net_age_scale, nets[income_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bit of uniformity seems also not impressive in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = make_regression(bucket_index, nets[income_index])\n",
    "make_plot(bucket_index, nets[income_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
