{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Actions:\n",
    "1. Missing values - remove another article.\n",
    "2. Dealing with categorical features - Bucketing bins, One-Hot encoding categorical\n",
    "2.1, 2.2 ...\n",
    "3 Feature transformation\n",
    "3.1 log\n",
    "3.2 feature expentions, polinoms, kernels,\n",
    "3.3 Normalization \n",
    "4. Feature crossing - (long, lat) date,\n",
    "5. Row aggregation, group by, \n",
    "5.1 Moving averages\n",
    "\n",
    "Future:\n",
    "6. Embedding - transformation, part of training. trainable/ non trainable, word2vec pre training. retrieve, train, seize, bottle neck tensor, calculate till you reach some training threshold, look up,\n",
    "\n",
    "Suggestions:\n",
    "# feature columns.\n",
    "integers categorical\n",
    "7. simple model improved after feature engineering.\n",
    "column name as variable,\n",
    "implement in TF, feature column, \n",
    "notebook out estimator \n",
    "models in tf\n",
    "panda -> convert to tf tf.dataset api\n",
    "use tf as possible for education\n",
    "put transformation in a map\n",
    "moving average in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project constants\n",
    "LEARN_SIZE = 50\n",
    "POLY_DEGREE = 2\n",
    "LEARNING_RATE = 0.001\n",
    "DISPLAY_STEP = 200\n",
    "NUM_STEPS = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Immediate execution\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "From Google Machine Learning crash course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems reasonable to evaluate also the dataset features. The dataset is very abundant, so we may limit the data only for the evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = housing_df[:LEARN_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a reasonable task to start from the data check. Is all data numerical? Does it contain missed values? Pandas can help us with a handy check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Shape before missing and corrupt values check: ', housing_df.shape)\n",
    "\n",
    "# Force number conversion\n",
    "for col in list(housing_df):\n",
    "    housing_df[col] = pd.to_numeric(housing_df[col], errors='coerce')\n",
    "\n",
    "# Remove intrinsic and resulted NaN values\n",
    "housing_df.dropna(inplace=True)\n",
    "\n",
    "print('Shape after missing and corrupt values check: ', housing_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, all values are appropriate.\n",
    "\n",
    "We can continue with the Tensorflow data processing now. We may prepare all source dataset columns (numeric) as Feature Columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(c) for c in list(housing_df)]\n",
    "features = [{c: tf.convert_to_tensor(housing_df[c])} for c in list(housing_df)]\n",
    "nets = [tf.feature_column.input_layer(features[i], feature_columns[i]) for i in range(len(feature_columns))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some numeric values perform better in the categorized form. For example, we can convert 'Housing median age' field into bucketing form. Such operation results in the one-hot-encoded matrix with the parameters separated into predefined bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_index = list(housing_df).index('housing_median_age')\n",
    "\n",
    "bucketized_column = tf.feature_column.bucketized_column(\n",
    "    source_column = feature_columns[age_index],\n",
    "    boundaries = [10, 20, 30, 40])\n",
    "\n",
    "net_age_bucket = tf.feature_column.input_layer(features[age_index], bucketized_column)\n",
    "print(net_age_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also a more condensed representation would help in our further calculations. This one-hot-decoding gives us a flat category (rank of current bin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_index = tf.reshape(tf.cast(tf.argmax(net_age_bucket, axis=1), tf.float32), [-1, 1])\n",
    "print(bucket_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow helps us to make a direct conversion of feature data during the import. For example, we obtain a log transformed data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transformer(x):\n",
    "    return tf.cast(tf.log(x), dtype=tf.float32)\n",
    "\n",
    "log_feature_column_age = tf.feature_column.numeric_column('housing_median_age', normalizer_fn=log_transformer)\n",
    "net_age_log = tf.feature_column.input_layer(features[age_index], log_feature_column_age)\n",
    "net_age_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data conversion may include a popular polynomial conversion. We will get the polynomially transformed (squared) data in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_transformer(x):\n",
    "    return tf.cast(tf.pow(x, POLY_DEGREE), dtype=tf.float32)\n",
    "\n",
    "poly_feature_column_age = tf.feature_column.numeric_column('housing_median_age', normalizer_fn=poly_transformer)\n",
    "net_age_poly = tf.feature_column.input_layer(features[age_index], poly_feature_column_age)\n",
    "net_age_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is one of the most popular data transformation methods. Tensorflow helps us to prepare a feature column with zero mean and uniform standard deviation easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mean = housing_df['housing_median_age'].mean()\n",
    "val_std = housing_df['housing_median_age'].std()\n",
    "\n",
    "def scaler(x):\n",
    "    return (tf.cast(x, dtype=tf.float32) - val_mean) / val_std\n",
    "\n",
    "scale_feature_column_age = tf.feature_column.numeric_column('housing_median_age', normalizer_fn=scaler)\n",
    "net_age_scale = tf.feature_column.input_layer(features[age_index], scale_feature_column_age)\n",
    "net_age_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to mention also more complex data processing methods like feature crossing. They could help us to combine similar variables like longitude and latitude for a further simultaneous processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantile_based_boundaries(feature_values, num_buckets):\n",
    "    boundaries = np.arange(1.0, num_buckets) / num_buckets\n",
    "    quantiles = feature_values.quantile(boundaries)\n",
    "    return [quantiles[q] for q in quantiles.keys()]\n",
    "\n",
    "longitude_bucket_feature_column = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column('longitude'),\n",
    "    boundaries=get_quantile_based_boundaries(housing_df['longitude'], 10))\n",
    "\n",
    "latitude_bucket_feature_column = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column('latitude'),\n",
    "    boundaries=get_quantile_based_boundaries(housing_df['latitude'], 10))\n",
    "\n",
    "crossed_lat_lon_feature_column = tf.feature_column.crossed_column(\n",
    "    [longitude_bucket_feature_column, latitude_bucket_feature_column], 50)\n",
    "\n",
    "net_longitude = tf.feature_column.input_layer(features[0], longitude_bucket_feature_column)\n",
    "net_latitude = tf.feature_column.input_layer(features[1], latitude_bucket_feature_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A detailed dataset evaluation may include grouping of some features. What if we want to estimate average dataset values related to the same housing mean age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.groupby(['housing_median_age']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise we may change the dataset representation by preparing the moving average to smooth possible divagations of variable variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df_sorted = housing_df.sort_values('housing_median_age')\n",
    "housing_df_sorted['h_m_age_rolling'] = housing_df_sorted['housing_median_age'].rolling(5, min_periods=1).mean()\n",
    "housing_df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some tests with our processed data. Does the feature processing actually improves the prediction?\n",
    "\n",
    "First we need to define a really simple linear regression using Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of linear equation\n",
    "a = tfe.Variable(np.random.randn())\n",
    "b = tfe.Variable(np.random.randn())\n",
    "\n",
    "def linear_regression(inputs):\n",
    "    return inputs * a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression objective as minimization of error\n",
    "def mean_square_fn(model_fn, inputs, labels):\n",
    "    n_samples = int(tf.size(labels))\n",
    "    return tf.reduce_sum(tf.pow(model_fn(inputs) - labels, 2)) / (2 * n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "grad = tfe.implicit_gradients(mean_square_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main regression routine\n",
    "def make_regression(x, y):\n",
    "    for step in range(NUM_STEPS):\n",
    "        optimizer.apply_gradients(grad(linear_regression, x, y))\n",
    "        if (step + 1) % DISPLAY_STEP == 0 or step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (step + 1), \"cost=\",\n",
    "                  \"{:.9f}\".format(mean_square_fn(linear_regression, x, y)),\n",
    "                  \"a=\", a.numpy(), \"b=\", b.numpy())\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "def make_plot(x, y):\n",
    "    plt.plot(x, y, 'ro', label='Original')\n",
    "    plt.plot(x, np.array(a * x + b), label='Fitted')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to estimate median income using the source median housing age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_index = list(housing_df).index('median_income')\n",
    "\n",
    "a, b = make_regression(nets[age_index], nets[income_index])\n",
    "make_plot(nets[age_index], nets[income_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is also a good try to improve the accuracy, but unfortunately not in this case..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = make_regression(net_age_scale, nets[income_index])\n",
    "make_plot(net_age_scale, nets[income_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bit of uniformity seems also not impressive in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = make_regression(bucket_index, nets[income_index])\n",
    "make_plot(bucket_index, nets[income_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
